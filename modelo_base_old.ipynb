{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IP57jvFo4Wh",
    "outputId": "c0bc6b35-82b2-4511-d8ca-2df2f87cd843"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'eda' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install ortools -q\n",
    "# download codes\n",
    "!git clone https://github.com/rilianx/eda.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmKUpDeVLUU3"
   },
   "source": [
    "## Creación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuJ9PC68ReLw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, head_dim, dropout_rate=0.2):\n",
    "        super(CustomModel, self).__init__()\n",
    "        #self.seq_length = seq_length  # Asumiendo una longitud fija de secuencia para simplificar\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # Proyección de entrada\n",
    "        self.input_projection = nn.Linear(input_dim, num_heads * head_dim)\n",
    "\n",
    "        # Capa de atención multi-cabeza\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=num_heads * head_dim,\n",
    "                                                         num_heads=num_heads,\n",
    "                                                         dropout=dropout_rate)\n",
    "\n",
    "        # Capas lineales individuales para cada posición de la secuencia\n",
    "        # Esto es un cambio respecto al código original para aplicar una capa lineal por posición de salida\n",
    "        self.positionwise_linear = nn.Linear(num_heads * head_dim, 1)\n",
    "\n",
    "        # Capa de salida final, después de un flatten, para aplicar Softmax\n",
    "        # Nota: Softmax se aplica después del flatten, por lo tanto no se define aquí como una capa pero sí en el forward\n",
    "\n",
    "    def generate_attention_mask(self, x, padding_value=0):\n",
    "        # Identificar posiciones de padding en x\n",
    "        mask = (x.sum(dim=-1) == padding_value)  # Asumiendo que el padding se puede identificar sumando los valores de la característica y comparando con 0\n",
    "        mask = mask.to(dtype=torch.bool)  # Convierte a bool para usar como máscara\n",
    "        # PyTorch espera una máscara con True y False donde True indica donde aplicar la máscara\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, x, seq_lengths=10, return_probabilities=False):\n",
    "        # x: [batch_size, seq_length, input_dim]\n",
    "        x = x.float()\n",
    "\n",
    "        max_len = x.shape[1]\n",
    "\n",
    "        # Generar máscara de atención basada en las longitudes de las secuencias\n",
    "        attn_mask = None\n",
    "\n",
    "        # Aplicar proyección de entrada\n",
    "        x_proj = self.input_projection(x)\n",
    "        x_proj = x_proj.permute(1, 0, 2)  # Reordenar para multihead_attention: [seq_length, batch_size, num_heads*head_dim]\n",
    "\n",
    "\n",
    "        # Aplicar atención multi-cabeza\n",
    "        attn_output, _ = self.multihead_attention(x_proj, x_proj, x_proj, attn_mask=attn_mask)\n",
    "        attn_output = attn_output.permute(1, 0, 2)  # Reordenar de vuelta: [batch_size, seq_length, num_heads*head_dim]\n",
    "\n",
    "        # Aplicar la capa lineal posición por posición\n",
    "        # Usamos una capa lineal que se aplica a cada vector de salida de la atención de forma independiente\n",
    "        positionwise_output = self.positionwise_linear(attn_output)\n",
    "\n",
    "        # Flatten\n",
    "        flat_output = positionwise_output.view(positionwise_output.size(0), -1)  # [batch_size, seq_length]\n",
    "\n",
    "        # Softmax\n",
    "        if return_probabilities:\n",
    "          output = F.softmax(flat_output, dim=-1)\n",
    "          return output\n",
    "        else: #return logits\n",
    "          return flat_output\n",
    "\n",
    "\n",
    "\n",
    "# Parámetros del modelo\n",
    "input_dim = 6  # Dimensión de la entrada\n",
    "num_heads = 10  # Número de cabezas en la atención multi-cabeza\n",
    "head_dim = 64  # Dimensión de cada cabeza\n",
    "\n",
    "\n",
    "# Crear el modelo\n",
    "model = CustomModel(input_dim=input_dim, num_heads=num_heads, head_dim=head_dim)\n",
    "\n",
    "# Información del modelo (Opcional)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDvnwDjCrOmY"
   },
   "source": [
    "## Generación de datos usando **Algoritmo clásico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HE3Hy8D98N8M"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "\n",
    "def distance(punto1, punto2):\n",
    "    return math.sqrt((punto1[0] - punto2[0])**2 + (punto1[1] - punto2[1])**2)\n",
    "\n",
    "# función para transformar un estado tsp en una secuencia de vectores\n",
    "# para el modelo basado en capas de atención\n",
    "def state2vecSeq(self):\n",
    "    # creamos dos diccionarios para mantenre un mapeo de los\n",
    "    # movimientos con los índices de la secuencia del modelo de aprendizaje\n",
    "\n",
    "    city_locations = self.inst_info.city_locations\n",
    "\n",
    "    idx2move = dict()\n",
    "    move2idx = dict()\n",
    "    origin = city_locations[self.visited[-1]]\n",
    "    destination = city_locations[self.visited[0]]\n",
    "\n",
    "    origin_dist = 0.0\n",
    "    dest_dist = distance(origin, destination)\n",
    "\n",
    "    seq = [list(origin) + [1,0] + [origin_dist, dest_dist], # Última ciudad visitada (origen)\n",
    "           list(destination) + [0, 1] + [dest_dist, 0.0]]  # Ciudad final\n",
    "\n",
    "    idx2move[0] = None\n",
    "    idx2move[1] = (\"constructive\", self.visited[0])\n",
    "    move2idx[self.visited[0]] = 1\n",
    "\n",
    "    idx = 2\n",
    "    for i in self.not_visited:\n",
    "        point = list(city_locations[i])\n",
    "        origin_dist = distance( point, origin)\n",
    "        dest_dist = distance( point, destination)\n",
    "        city_vector = point + [0, 0] + [origin_dist, dest_dist] # Otras ciudades\n",
    "\n",
    "        seq.append(city_vector)\n",
    "        idx2move[idx] = (\"constructive\", i)\n",
    "        move2idx[i] = idx\n",
    "        idx += 1\n",
    "\n",
    "    return seq, idx2move, move2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7UgboEfrQNN",
    "outputId": "c2014eb3-7f28-48ce-e069-944723bb1862"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn.functional import one_hot\n",
    "from eda.TSP import TSP_Instance, TSP_Environment, TSP_State\n",
    "from eda.solveTSP_v2 import solve\n",
    "env = TSP_Environment\n",
    "\n",
    "def generate_data(max_cities=20, nb_sample=100):\n",
    "    X = []  # Lista para almacenar las secuencias de entrada\n",
    "    Y = []  # Lista para almacenar las etiquetas objetivo (las siguientes ciudades a visitar)\n",
    "    seq_len = max_cities + 1  # Longitud de la secuencia, ajustada para incluir una ciudad extra\n",
    "\n",
    "    # Bucle para generar datos hasta alcanzar el número deseado de muestras\n",
    "    while True:\n",
    "        # 1. Generamos instancia aleatoria\n",
    "        n_cities = max_cities\n",
    "        dim = 2  # Dimensión para las coordenadas de la ciudad (2D: x, y)\n",
    "        city_points = np.random.rand(n_cities, dim)  # Generar puntos aleatorios para las ciudades\n",
    "        inst_info = TSP_Instance(city_points)\n",
    "\n",
    "        # 2. Resolvemos TSP usando algoritmo tradicional\n",
    "        solution = solve(city_points)  # Resolver el TSP y obtener un estado final\n",
    "\n",
    "        # 3. Iteramos sobre los movimientos de la solución final para generar varias muestras:\n",
    "        # estado (X) -> movimiento (Y)\n",
    "        current_state = TSP_State (inst_info)\n",
    "        env.state_transition(current_state, (\"constructive\",solution.visited[0]))\n",
    "        samples_per_sol = max_cities-1  # Número máximo de muestras por solución\n",
    "        for move in [(\"constructive\", city) for city in solution.visited[1:]]:\n",
    "            seq, _, move2idx = state2vecSeq(current_state)  # Convertir el estado actual a secuencia vectorizada\n",
    "\n",
    "            X.append(torch.tensor(seq))  # Añadir la secuencia a X\n",
    "            Y.append(one_hot(torch.tensor(move2idx[move[1]]), num_classes=seq_len))\n",
    "            #Y.append(to_categorical(move2idx[move[1]], num_classes=seq_len))  # Añadir el movimiento como categoría a Y\n",
    "\n",
    "            env.state_transition(current_state, move)  # Hacer la transición al siguiente estado\n",
    "\n",
    "            # Condiciones de parada basadas en el número de ciudades visitadas/no visitadas o muestras generadas\n",
    "            if len(current_state.visited) > samples_per_sol or len(X) >= nb_sample:\n",
    "                break\n",
    "\n",
    "        # Romper el bucle externo si se ha alcanzado el número deseado de muestras\n",
    "        if len(X) >= nb_sample:\n",
    "            break\n",
    "\n",
    "    X_padded = torch.nn.utils.rnn.pad_sequence(X, batch_first=True)\n",
    "\n",
    "    return X_padded, torch.stack(Y)\n",
    "\n",
    "generate_data(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QI0a7f0Cz7to",
    "outputId": "d1548faa-5eb0-496c-ceec-4c9cf982b95f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X,Y=generate_data(max_cities=10, nb_sample=20000)\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNX7U1g4RUMg"
   },
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2MZUy9CXKMa",
    "outputId": "f9ca57c6-1480-4af9-8ff7-4250e12623f9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "\n",
    "# Asumiendo que X_padded y Y_stacked ya están definidos y son tensores de PyTorch\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba}\n",
    "train_size = int(0.5 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Definir el modelo, la función de pérdida y el optimizador\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Ciclo de entrenamiento\n",
    "epochs = 5\n",
    "# Initialize the DataFrame to store training results\n",
    "df = pd.DataFrame(columns=[\"cities\", \"iter\", \"Epoch\",\n",
    "                           \"Training Loss\", \"Training Accuracy\",\n",
    "                           \"Validation Loss\", \"Validation Accuracy\"])\n",
    "for num_iter in range(10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0; total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Limpia los gradientes\n",
    "            outputs = model(X_batch)  # Obtenemos logits\n",
    "            loss = loss_function(outputs, y_batch.argmax(dim=1))  # Calcular la pérdida\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Actualizar parámetros\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch.argmax(dim=1)).sum().item()\n",
    "\n",
    "    \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = 100 * correct / total\n",
    "    \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        correct = 0; total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_function(outputs, y_batch.argmax(dim=1))\n",
    "                validation_loss += loss.item() * X_batch.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch.argmax(dim=1)).sum().item()\n",
    "        validation_loss /= len(test_loader.dataset)\n",
    "        validation_accuracy = 100 * correct / total\n",
    "        df = pd.concat([df, pd.DataFrame([{\n",
    "            \"cities\": 50,\n",
    "            \"iter\": num_iter,\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Training Loss\": train_loss,\n",
    "            \"Training Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": validation_loss,\n",
    "            \"Validation Accuracy\": validation_accuracy\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'Epoch {epoch+1}, Val Loss: {validation_loss:.4f}, Val Accuracy: {validation_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD-kS8FhMSxt"
   },
   "source": [
    "## Validación del modelo.\n",
    "Se utiliza como función de evaluación de movimientos dentro de algoritmo constructivo Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cs7Bc7k64KG4",
    "outputId": "aecc41c5-0005-48f1-b8d6-e517adb6d5ae"
   },
   "outputs": [],
   "source": [
    "from eda.TSP import TSP_Instance, TSP_Environment, TSP_State, evalConstructiveActions, plot_tour\n",
    "from eda.agents import SingleAgentSolver, GreedyAgent\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#with open('tsp_model.pkl', 'rb') as archivo:\n",
    "#    model = pickle.load(archivo)\n",
    "\n",
    "class ModelEvalActions():\n",
    "  def __init__(self, model):\n",
    "    self.model=model\n",
    "\n",
    "  # permite evaluar acctiones de varios estados a la vez\n",
    "  # para optimizar los cáluclos del modelo\n",
    "  def __call__(self, states, env):\n",
    "    single_state = False\n",
    "    if not isinstance(states, list):\n",
    "      single_state=True\n",
    "      states = [states]\n",
    "\n",
    "    evals = [list() for _ in states]\n",
    "    vecSeqs=[]; move2idx =[]\n",
    "\n",
    "    for state in states:\n",
    "      vecSeq, _, mov2idx = state.state2vecSeq()\n",
    "      vecSeqs.append(vecSeq)\n",
    "      move2idx.append(mov2idx)\n",
    "\n",
    "    predictions = self.model(torch.tensor(vecSeqs), return_probabilities=True)\n",
    "\n",
    "    for k in range(len(states)):\n",
    "      state = states[k]\n",
    "      for action in env.gen_actions(state, \"constructive\"):\n",
    "          idx = move2idx[k][action[1]] #mapping from move to output index (model)\n",
    "          evals[k].append((action,predictions[k][idx]))\n",
    "\n",
    "    if single_state: return evals[0]\n",
    "    else: return evals\n",
    "\n",
    "# np.random.seed(42)\n",
    "\n",
    "# creamos un problema con 50 ciudades en un plano 2D\n",
    "cities  = np.random.rand(500, 2)\n",
    "inst_info = TSP_Instance(cities)\n",
    "\n",
    "# referenciamos nuestro ambiente con las \"reglas del juego\"\n",
    "env = TSP_Environment\n",
    "# creamos nuestro agente\n",
    "greedy = SingleAgentSolver (env,GreedyAgent(ModelEvalActions(model)))\n",
    "solution, *_ = greedy.solve(TSP_State (inst_info, visited=[0]))\n",
    "print(\"Model solution:\\n\", solution)\n",
    "plot_tour(cities, solution.visited)\n",
    "\n",
    "\n",
    "greedy2 = SingleAgentSolver (env,GreedyAgent(evalConstructiveActions))\n",
    "solution, *_ = greedy2.solve(TSP_State (inst_info, visited=[0]))\n",
    "print(\"Greedy solution:\\n\", solution)\n",
    "plot_tour(cities, solution.visited)\n",
    "\n",
    "solution = solve(cities)\n",
    "print (\"OR-Tools:\",solution.cost)\n",
    "plot_tour(cities, solution.visited)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlkZa2J5XHSN"
   },
   "source": [
    "### Múltiples ejecuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "jwkGPpCAXmbG",
    "outputId": "526a688a-a608-4413-f44a-d46be29d7353"
   },
   "outputs": [],
   "source": [
    "from eda.agents import StochasticGreedyAgent\n",
    "\n",
    "greedy = SingleAgentSolver (env,StochasticGreedyAgent(ModelEvalActions(model), steepness=50))\n",
    "solutions, *_ = greedy.multistate_solve([deepcopy(TSP_State (inst_info, visited=[0])) for _ in range(10)])\n",
    "\n",
    "print([s.cost for s in solutions])\n",
    "\n",
    "best_sol = min(solutions, key=lambda solution: solution.cost)\n",
    "print(\"Best solution:\\n\", best_sol)\n",
    "plot_tour(cities, best_sol.visited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScYBPTlsg2jx"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"base_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "nb_eval = 50\n",
    "fold = 10\n",
    "eval_df = pd.DataFrame(columns=[\"model_name\", \"cost\"])\n",
    "instances = [\n",
    "                TSP_Instance(np.random.rand(50, 2)) for _ in tqdm(\n",
    "                    range(nb_eval), desc=\"Instances\", unit=\"instance\", position=0, leave=True\n",
    "                )\n",
    "            ]\n",
    "greedy = SingleAgentSolver(env, GreedyAgent(ModelEvalActions(model)))\n",
    "solutions = []\n",
    "\n",
    "for instance in tqdm(instances, desc=\"Solving Instances\", unit=\"instance\", position=0, leave=True):\n",
    "    solution, *_ = greedy.solve(TSP_State(instance, visited=[0]))\n",
    "    eval_df = pd.concat([eval_df, pd.DataFrame([{\n",
    "        \"model_name\" : type(model).__name__,\n",
    "        \"iter\" : iter_num,        \n",
    "        \"cost\": solution.cost,                \n",
    "}]) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_df.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
