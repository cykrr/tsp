{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31962b4c-ce20-43e5-b4af-412b20b6a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af147a-a740-4eb4-8053-0ba0630e1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn.functional import one_hot\n",
    "from eda.TSP import TSP_Instance, TSP_Environment, TSP_State\n",
    "from eda.solveTSP_v2 import solve\n",
    "\n",
    "def generate_data(n_cities=50, nb_sample=512):\n",
    "    X = []\n",
    "    Y = []\n",
    "    while len(X) < nb_sample:\n",
    "        city_points = np.random.rand(n_cities, 2)\n",
    "        inst_info = TSP_Instance(city_points)\n",
    "        solution = solve(city_points)\n",
    "\n",
    "        X.append(torch.from_numpy(city_points))\n",
    "        Y.append(torch.tensor(solution.visited))\n",
    "    return torch.stack(X).float(), torch.stack(Y).float()\n",
    "\n",
    "X, Y = generate_data()\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa5a6a-59b2-4449-b490-2504454f8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_dim = 6  # Dimensión de la entrada\n",
    "        self.num_heads = 16  # Número de cabezas en la atención multi-cabeza\n",
    "        self.head_dim = 8  # Dimensión de cada cabeza\n",
    "        self.node_dim = 2\n",
    "        self.embd_dim = self.num_heads * self.head_dim\n",
    "        self.ff_dim=256\n",
    "        self.nb_layers=6\n",
    "        self.batchnorm=True\n",
    "        \n",
    "        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(self.embd_dim, self.num_heads) for _ in range(self.nb_layers)] )\n",
    "        self.linear1_layers = nn.ModuleList( [nn.Linear(self.embd_dim, self.ff_dim) for _ in range(self.nb_layers)] )\n",
    "        self.linear2_layers = nn.ModuleList( [nn.Linear(self.ff_dim, self.embd_dim) for _ in range(self.nb_layers)] )   \n",
    "        if self.batchnorm:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(self.embd_dim) for _ in range(self.nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(self.embd_dim) for _ in range(self.nb_layers)] )\n",
    "        else:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(self.embd_dim) for _ in range(self.nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(self.embd_dim) for _ in range(self.nb_layers)] )\n",
    "\n",
    "        self.norm = nn.BatchNorm1d(self.embd_dim)\n",
    "    def forward(self, h):      \n",
    "        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n",
    "        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n",
    "        # L layers\n",
    "        for i in range(self.nb_layers):\n",
    "            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n",
    "            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n",
    "            # add residual connection\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n",
    "            # feedforward\n",
    "            h_rc = h # residual connection\n",
    "            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "        # Transpose h\n",
    "        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        return h, score\n",
    "\n",
    "def generate_positional_encoding(d_model, max_len):\n",
    "    \"\"\"\n",
    "    Create standard transformer PEs.\n",
    "    Inputs :  \n",
    "      d_model is a scalar correspoding to the hidden dimension\n",
    "      max_len is the maximum length of the sequence\n",
    "    Output :  \n",
    "      pe of size (max_len, d_model), where d_model=dim_emb, max_len=1000\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Parámetros del modelo\n",
    "        self.input_dim = 6  # Dimensión de la entrada\n",
    "        self.num_heads = 16  # Número de cabezas en la atención multi-cabeza\n",
    "        self.head_dim = 8  # Dimensión de cada cabeza\n",
    "        self.node_dim = 2\n",
    "        self.embd_dim = self.num_heads * self.head_dim\n",
    "        self.ff_dim=256\n",
    "        self.nb_dec_layers = 2\n",
    "        self.nb_nodes=50\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Parámetros del modelo\n",
    "        self.input_dim = 6  # Dimensión de la entrada\n",
    "        self.num_heads = 16  # Número de cabezas en la atención multi-cabeza\n",
    "        self.head_dim = 8  # Dimensión de cada cabeza\n",
    "        self.node_dim = 2\n",
    "        self.embd_dim = self.num_heads * self.head_dim\n",
    "        self.ff_dim=256\n",
    "        self.nb_dec_layers = 2\n",
    "        self.nb_nodes=50\n",
    "        max_len_PE = 10000\n",
    "\n",
    "        self.input_emb = nn.Linear(self.node_dim, self.embd_dim)\n",
    "        self.ff = nn.Linear(self.embd_dim, 1)\n",
    "        self.start_placeholder = nn.Parameter(torch.randn(self.embd_dim))\n",
    "\n",
    "        self.enc = Encoder()\n",
    "        \n",
    "        self.WKatt_dec = nn.Linear(self.embd_dim, self.nb_dec_layers * self.embd_dim)\n",
    "        self.WVatt_dec = nn.Linear(self.embd_dim, self.nb_dec_layers * self.embd_dim)\n",
    "        self.PE = generate_positional_encoding(self.embd_dim, max_len_PE)     \n",
    "\n",
    "        self.mha= nn.MultiheadAttention(self.embd_dim, self.num_heads)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "    def forward(self, x, return_probabilities=False):\n",
    "        # x: (bsz, nb_nodes, dim)\n",
    "        zero_to_bsz = torch.arange(x.shape[0])\n",
    "        bsz = x.shape[0]\n",
    "\n",
    "        attn_mask = None\n",
    "        h = self.input_emb(x)\n",
    "        \n",
    "        repeated_placeholder = self.start_placeholder.repeat(bsz, 1, 1)\n",
    "        h = torch.cat([h, repeated_placeholder ], dim=1)\n",
    "\n",
    "        h_enc, _ = self.enc(h)\n",
    "        tours = []\n",
    "        sumLog = []\n",
    "        Katt_dec = self.WKatt_dec(h_enc)\n",
    "        Vatt_dec = self.WVatt_dec(h_enc)\n",
    "        self.PE = self.PE.to(x.device)\n",
    "        idx_start_placeholder = torch.tensor([self.nb_nodes]).long().repeat(bsz).to(x.device)\n",
    "        h_start = h_enc[zero_to_bsz, idx_start_placeholder, :] + self.PE[0].repeat(bsz, 1) \n",
    "\n",
    "        mask_visited_nodes = torch.zeros(bsz, self.nb_nodes +1, device = x.device)\n",
    "        mask_visited_nodes[zero_to_bsz, idx_start_placeholder] = True\n",
    "\n",
    "        h_t = h_start\n",
    "\n",
    "        for t in range(self.nb_nodes):\n",
    "            prob_next_node, _ = self.mha(h_t, Katt_dec, Vatt_dec, mask_visited_nodes)\n",
    "            if deterministic:\n",
    "                idx = torch.argmax(prob_next_node, dim=1)\n",
    "            else:\n",
    "                idx = Categorical(prob_next_node).sample()\n",
    "            \n",
    "        return h        \n",
    "\n",
    "\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f028e-5c02-4492-9a17-70b13df65da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel()\n",
    "model(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132a083-03dc-4811-8ed9-94357f390010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7aa97-9051-4444-9cc9-5b612b163505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
